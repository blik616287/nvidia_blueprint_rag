# NVIDIA RAG Core NIMs Pack Values
# Version 1.0.3 - Dynamic Model Selection
#
# This pack automatically detects available GPU memory and selects
# the appropriate Llama model tier:
#
# | Total VRAM    | Model Selected              | GPUs |
# |---------------|-----------------------------| -----|
# | 16-24 GB      | Llama 3.1 8B (FP16)        | 1    |
# | 25-47 GB      | Llama 3.1 8B (Enhanced)    | 1    |
# | 48-79 GB      | Llama 3.1 70B (TP=2)       | 2    |
# | 80-159 GB     | Llama 3.1 70B (TP=4)       | 4    |
# | 160+ GB       | Nemotron 49B (TP=4)        | 4    |

pack:
  namespace: nvidia-rag
  spectrocloud.com/install-priority: "11"
  content:
    images:
      # Dynamic model selection images (Llama family)
      - image: nvcr.io/nim/meta/llama-3.1-8b-instruct:latest
      - image: nvcr.io/nim/meta/llama-3.1-70b-instruct:latest
      - image: nvcr.io/nim/nvidia/llama-3.3-nemotron-super-49b-v1.5:latest
      # Embedding and Reranker
      - image: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2:latest
      - image: nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2:latest
      # GPU detection init container
      - image: nvcr.io/nvidia/cuda:12.4.0-base-ubuntu22.04

# Helm chart values
charts:
  nvidia-rag-core-nims:
    # Global settings
    global:
      namespace: "nvidia-rag"
      storageClass: ""  # Uses cluster default storage class
      ngcApiKey: "{{.spectro.var.NGC_API_KEY}}"
      nvidiaApiKey: "{{.spectro.var.NVIDIA_API_KEY}}"  # For cloud NIM APIs (ai.api.nvidia.com)
      imagePullSecrets:
        - name: ngc-pull-secret

    # NGC Pull Secret Configuration
    ngcPullSecret:
      create: true
      name: "ngc-pull-secret"
      registry: "nvcr.io"
      username: "$oauthtoken"

    # NGC API Secret (for cloud NIMs)
    ngcApiSecret:
      create: true
      name: "ngc-api-key"

    # LLM NIM Configuration with Dynamic Model Selection
    llm:
      enabled: true
      useCloud: false
      cloudEndpoint: "https://integrate.api.nvidia.com/v1"

      # ============================================================
      # Model Selection Priority (NEW in 1.0.3):
      # 1. Manual Override (if manualOverride.enabled=true)
      # 2. Dynamic Selection (if dynamicModelSelection.enabled=true)
      # 3. Fallback to Llama 3.1 8B
      # ============================================================

      # Manual Override - Takes precedence over everything
      # Use this when you want to specify an exact model for your deployment
      manualOverride:
        enabled: false
        # Specify the exact model to use (only applied if enabled=true)
        model:
          name: "meta/llama-3.1-8b-instruct"
          image: "nvcr.io/nim/meta/llama-3.1-8b-instruct"
          tag: "latest"
          gpuCount: 1
          tensorParallelism: 1
          precision: "fp16"

      # Dynamic Model Selection - Auto-detect GPU and select best model
      # Only used if manualOverride.enabled=false
      dynamicModelSelection:
        enabled: true
        # Maximum number of GPUs to use for the LLM model
        # Set to 1 for single-GPU deployments (default)
        # Set to 2, 4, or 8 to enable multi-GPU tensor parallelism
        maxGpuCount: 1
        # Default/fallback model (Llama 3.1 8B - fits on any 16GB+ GPU)
        defaultModel: "meta/llama-3.1-8b-instruct"
        defaultImage: "nvcr.io/nim/meta/llama-3.1-8b-instruct"
        defaultTag: "latest"
        # GPU detection container
        detectorImage: "nvcr.io/nvidia/cuda:12.4.0-base-ubuntu22.04"

      nimService:
        name: "nim-llm"
        image:
          # Used when dynamicModelSelection.enabled is false
          repository: "nvcr.io/nim/meta/llama-3.1-8b-instruct"
          tag: "latest"
          pullPolicy: IfNotPresent
        model:
          name: "meta/llama-3.1-8b-instruct"
          engine: "tensorrt_llm"
          precision: "fp16"
          qosProfile: "throughput"
        resources:
          requests:
            memory: "16Gi"
            cpu: "4"
          limits:
            memory: "32Gi"
            cpu: "8"
            nvidia.com/gpu: 1
        storage:
          pvc:
            create: true
            storageClass: ""
            size: "100Gi"  # Sized for largest possible model
            accessMode: "ReadWriteOnce"
          sharedMemorySizeLimit: "16Gi"
        service:
          type: ClusterIP
          port: 8000
        healthCheck:
          initialDelaySeconds: 1800
          periodSeconds: 30
          timeoutSeconds: 10
        nodeSelector:
          nvidia.com/gpu.present: "true"
        tolerations:
          - key: "nvidia.com/gpu"
            operator: "Exists"
            effect: "NoSchedule"
        env:
          - name: NIM_HTTP_API_PORT
            value: "8000"
          - name: NIM_LOG_LEVEL
            value: "INFO"

    # Embedding NIM Configuration
    embedding:
      enabled: true
      useCloud: false
      cloudEndpoint: "https://integrate.api.nvidia.com/v1"
      nimService:
        name: "nemoretriever-embedding-ms"
        image:
          repository: "nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2"
          tag: "latest"
          pullPolicy: IfNotPresent
        model:
          name: "nvidia/llama-3.2-nv-embedqa-1b-v2"
          dimensions: 2048
        resources:
          requests:
            memory: "8Gi"
            cpu: "4"
          limits:
            memory: "16Gi"
            cpu: "8"
            nvidia.com/gpu: 1
        storage:
          pvc:
            create: true
            storageClass: ""
            size: "50Gi"
            accessMode: "ReadWriteOnce"
          sharedMemorySizeLimit: "16Gi"
        service:
          type: ClusterIP
          port: 8000
        healthCheck:
          initialDelaySeconds: 120
          periodSeconds: 15
          timeoutSeconds: 5
        nodeSelector:
          nvidia.com/gpu.present: "true"
        tolerations:
          - key: "nvidia.com/gpu"
            operator: "Exists"
            effect: "NoSchedule"
        env:
          - name: NIM_HTTP_API_PORT
            value: "8000"
          - name: NIM_TRITON_PERFORMANCE_MODE
            value: "throughput"

    # Reranker NIM Configuration
    reranker:
      enabled: true
      useCloud: false
      cloudEndpoint: "https://integrate.api.nvidia.com/v1"
      nimService:
        name: "nemoretriever-ranking-ms"
        image:
          repository: "nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2"
          tag: "latest"
          pullPolicy: IfNotPresent
        model:
          name: "nvidia/llama-3.2-nv-rerankqa-1b-v2"
        resources:
          requests:
            memory: "8Gi"
            cpu: "4"
          limits:
            memory: "16Gi"
            cpu: "8"
            nvidia.com/gpu: 1
        storage:
          pvc:
            create: true
            storageClass: ""
            size: "50Gi"
            accessMode: "ReadWriteOnce"
          sharedMemorySizeLimit: "16Gi"
        service:
          type: ClusterIP
          port: 8000
        healthCheck:
          initialDelaySeconds: 120
          periodSeconds: 15
          timeoutSeconds: 5
        nodeSelector:
          nvidia.com/gpu.present: "true"
        tolerations:
          - key: "nvidia.com/gpu"
            operator: "Exists"
            effect: "NoSchedule"
        env:
          - name: NIM_HTTP_API_PORT
            value: "8000"
