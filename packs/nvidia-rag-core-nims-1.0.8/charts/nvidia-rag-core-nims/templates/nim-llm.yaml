{{- if and .Values.llm.enabled (not .Values.llm.useCloud) }}
{{- $useManualOverride := .Values.llm.manualOverride.enabled }}
{{- $useDynamicSelection := and (not $useManualOverride) .Values.llm.dynamicModelSelection.enabled }}
{{- $useStaticConfig := and (not $useManualOverride) (not $useDynamicSelection) }}
---
# LLM NIM PersistentVolumeClaim (Model Cache)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: {{ .Values.llm.nimService.name }}-cache
  namespace: {{ .Values.global.namespace }}
  labels:
    app.kubernetes.io/name: {{ .Values.llm.nimService.name }}
    app.kubernetes.io/instance: rag
    app.kubernetes.io/component: llm
spec:
  accessModes:
    - {{ .Values.llm.nimService.storage.pvc.accessMode }}
  storageClassName: {{ .Values.llm.nimService.storage.pvc.storageClass | default .Values.global.storageClass }}
  resources:
    requests:
      storage: {{ .Values.llm.nimService.storage.pvc.size }}
---
# LLM NIM Deployment
# Model Selection Mode: {{ if $useManualOverride }}Manual Override{{ else if $useDynamicSelection }}Dynamic Selection{{ else }}Static Config{{ end }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Values.llm.nimService.name }}
  namespace: {{ .Values.global.namespace }}
  labels:
    app.kubernetes.io/name: {{ .Values.llm.nimService.name }}
    app.kubernetes.io/instance: rag
    app.kubernetes.io/component: llm
    nvidia.com/nim: "true"
  annotations:
    spectrocloud.com/model-selection-mode: {{ if $useManualOverride }}"manual"{{ else if $useDynamicSelection }}"dynamic"{{ else }}"static"{{ end }}
    {{- if $useManualOverride }}
    spectrocloud.com/manual-model: "{{ .Values.llm.manualOverride.model.name }}"
    {{- end }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ .Values.llm.nimService.name }}
      app.kubernetes.io/instance: rag
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ .Values.llm.nimService.name }}
        app.kubernetes.io/instance: rag
        app.kubernetes.io/component: llm
        nvidia.com/nim: "true"
    spec:
      {{- with .Values.global.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.llm.nimService.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.llm.nimService.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      securityContext:
        fsGroup: 1000
      {{- if $useDynamicSelection }}
      # Init container for GPU detection and dynamic model selection
      # Only used when manualOverride.enabled=false AND dynamicModelSelection.enabled=true
      initContainers:
        - name: gpu-detect
          image: {{ .Values.llm.dynamicModelSelection.detectorImage | default "nvcr.io/nvidia/cuda:12.4.0-base-ubuntu22.04" }}
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - /scripts/detect-and-select.sh
          env:
            - name: NGC_API_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.ngcApiSecret.name }}
                  key: NGC_API_KEY
            - name: MAX_GPU_COUNT
              value: "{{ .Values.llm.dynamicModelSelection.maxGpuCount | default 1 }}"
          volumeMounts:
            - name: detect-script
              mountPath: /scripts
            - name: model-tiers
              mountPath: /config
            - name: model-config
              mountPath: /model-config
          resources:
            requests:
              nvidia.com/gpu: 1
              memory: "1Gi"
              cpu: "500m"
            limits:
              nvidia.com/gpu: 1
              memory: "2Gi"
              cpu: "1"
      {{- end }}
      containers:
        - name: nim-llm
          {{- if $useManualOverride }}
          # Manual Override Mode - Using explicitly specified model
          image: "{{ .Values.llm.manualOverride.model.image }}:{{ .Values.llm.manualOverride.model.tag }}"
          {{- else if $useDynamicSelection }}
          # Dynamic Selection Mode - Using default/fallback image, model selected at runtime
          image: "{{ .Values.llm.dynamicModelSelection.defaultImage }}:{{ .Values.llm.dynamicModelSelection.defaultTag }}"
          {{- else }}
          # Static Config Mode - Using nimService.image configuration
          image: "{{ .Values.llm.nimService.image.repository }}:{{ .Values.llm.nimService.image.tag }}"
          {{- end }}
          imagePullPolicy: {{ .Values.llm.nimService.image.pullPolicy }}
          env:
            - name: NGC_API_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.ngcApiSecret.name }}
                  key: NGC_API_KEY
            {{- if $useManualOverride }}
            # Manual Override - Using specified model
            - name: NIM_SERVED_MODEL_NAME
              value: "{{ .Values.llm.manualOverride.model.name }}"
            {{- if .Values.llm.manualOverride.model.tensorParallelism }}
            - name: NIM_TENSOR_PARALLEL_SIZE
              value: "{{ .Values.llm.manualOverride.model.tensorParallelism }}"
            {{- end }}
            {{- else if $useDynamicSelection }}
            # Dynamic Selection - Default model (may be overridden by init container)
            - name: NIM_MODEL_NAME
              value: "{{ .Values.llm.dynamicModelSelection.defaultModel }}"
            {{- else }}
            # Static Config - Using nimService model name
            - name: NIM_SERVED_MODEL_NAME
              value: "{{ .Values.llm.nimService.model.name }}"
            {{- end }}
            {{- range .Values.llm.nimService.env }}
            - name: {{ .name }}
              value: {{ .value | quote }}
            {{- end }}
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
            - name: metrics
              containerPort: 8002
              protocol: TCP
          resources:
            {{- if $useManualOverride }}
            requests:
              memory: "{{ .Values.llm.nimService.resources.requests.memory }}"
              cpu: "{{ .Values.llm.nimService.resources.requests.cpu }}"
            limits:
              memory: "{{ .Values.llm.nimService.resources.limits.memory }}"
              cpu: "{{ .Values.llm.nimService.resources.limits.cpu }}"
              nvidia.com/gpu: {{ .Values.llm.manualOverride.model.gpuCount | default 1 }}
            {{- else }}
            {{- toYaml .Values.llm.nimService.resources | nindent 12 }}
            {{- end }}
          volumeMounts:
            - name: model-cache
              mountPath: /opt/nim/.cache
            - name: dshm
              mountPath: /dev/shm
            {{- if $useDynamicSelection }}
            - name: model-config
              mountPath: /model-config
              readOnly: true
            {{- end }}
          livenessProbe:
            httpGet:
              path: /v1/health/live
              port: http
            initialDelaySeconds: {{ .Values.llm.nimService.healthCheck.initialDelaySeconds }}
            periodSeconds: {{ .Values.llm.nimService.healthCheck.periodSeconds }}
            timeoutSeconds: {{ .Values.llm.nimService.healthCheck.timeoutSeconds }}
          readinessProbe:
            httpGet:
              path: /v1/health/ready
              port: http
            initialDelaySeconds: 60
            periodSeconds: 15
            timeoutSeconds: 10
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: {{ .Values.llm.nimService.name }}-cache
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: {{ .Values.llm.nimService.storage.sharedMemorySizeLimit }}
        {{- if $useDynamicSelection }}
        - name: detect-script
          configMap:
            name: gpu-detect-script
            defaultMode: 0755
        - name: model-tiers
          configMap:
            name: llm-model-tiers
        - name: model-config
          emptyDir: {}
        {{- end }}
---
# LLM NIM Service
apiVersion: v1
kind: Service
metadata:
  name: {{ .Values.llm.nimService.name }}
  namespace: {{ .Values.global.namespace }}
  labels:
    app.kubernetes.io/name: {{ .Values.llm.nimService.name }}
    app.kubernetes.io/instance: rag
    app.kubernetes.io/component: llm
spec:
  type: {{ .Values.llm.nimService.service.type }}
  ports:
    - name: http
      port: {{ .Values.llm.nimService.service.port }}
      targetPort: http
      protocol: TCP
    - name: metrics
      port: 8002
      targetPort: metrics
      protocol: TCP
  selector:
    app.kubernetes.io/name: {{ .Values.llm.nimService.name }}
    app.kubernetes.io/instance: rag
{{- end }}

{{- if .Values.llm.useCloud }}
---
# LLM Cloud Endpoint ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-cloud-config
  namespace: {{ .Values.global.namespace }}
  labels:
    app.kubernetes.io/name: llm-cloud
    app.kubernetes.io/instance: rag
data:
  LLM_ENDPOINT: {{ .Values.llm.cloudEndpoint | quote }}
  LLM_MODEL: {{ .Values.llm.nimService.model.name | quote }}
{{- end }}
